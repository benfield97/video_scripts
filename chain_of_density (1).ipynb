{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b02a1c74ba34b5499ee64ff269335dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "YouTube URL:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1bd48875a1c74f5687482bd218756d0e",
            "placeholder": "Enter YouTube URL",
            "style": "IPY_MODEL_b23d80111ad8426fafe433733009a016",
            "value": "https://www.youtube.com/watch?v=i-oHvHejdsc"
          }
        },
        "1bd48875a1c74f5687482bd218756d0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23d80111ad8426fafe433733009a016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "651f0707d28044faa7af58e1afe5854e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Download",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ccf19dd5d41b465790942e1f7c5955d5",
            "style": "IPY_MODEL_4e0ef05c28744fd7955858a2be71e604",
            "tooltip": ""
          }
        },
        "ccf19dd5d41b465790942e1f7c5955d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e0ef05c28744fd7955858a2be71e604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2c4140147f3f481084bd207ef5d9711b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7b997c88fa44252bdec96b20bac5d8b",
              "IPY_MODEL_ad2d22217083437781b41a282e308636"
            ],
            "layout": "IPY_MODEL_c0e1057d20994a57a4aa5bff5fe0a9fa"
          }
        },
        "e7b997c88fa44252bdec96b20bac5d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "API Key:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0f0a83b152d4444f82efd70514a4c095",
            "placeholder": "Enter your OpenAI API key",
            "style": "IPY_MODEL_24c50f08a2b547659b4d02d171cdc664",
            "value": "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"
          }
        },
        "ad2d22217083437781b41a282e308636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_30fb1c3bce00479a867997ded85b5fa0",
            "style": "IPY_MODEL_22a9dde7c8444b4ba80183ae072678cb",
            "tooltip": ""
          }
        },
        "c0e1057d20994a57a4aa5bff5fe0a9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0a83b152d4444f82efd70514a4c095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c50f08a2b547659b4d02d171cdc664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fb1c3bce00479a867997ded85b5fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a9dde7c8444b4ba80183ae072678cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_WxQGMMRiT9"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain moviepy pydub py-llm-core pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Video\n"
      ],
      "metadata": {
        "id": "0KPr4QuflBKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "from pytube import YouTube\n",
        "\n",
        "text = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter YouTube URL',\n",
        "    description='YouTube URL:',\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Download\")\n",
        "\n",
        "def download_action(b):\n",
        "    link = text.value\n",
        "    youtubeObject = YouTube(link)\n",
        "    youtubeObject = youtubeObject.streams.get_highest_resolution()\n",
        "    try:\n",
        "        youtubeObject.download()\n",
        "        print(\"Download is completed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error has occurred: {e}\")\n",
        "\n",
        "button.on_click(download_action)\n",
        "\n",
        "display(text, button)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "1b02a1c74ba34b5499ee64ff269335dc",
            "1bd48875a1c74f5687482bd218756d0e",
            "b23d80111ad8426fafe433733009a016",
            "651f0707d28044faa7af58e1afe5854e",
            "ccf19dd5d41b465790942e1f7c5955d5",
            "4e0ef05c28744fd7955858a2be71e604"
          ]
        },
        "id": "Nxw1Hvr9lHK0",
        "outputId": "40f47020-8571-412b-e9e4-437be4ebb257"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='YouTube URL:', placeholder='Enter YouTube URL')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b02a1c74ba34b5499ee64ff269335dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Download', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "651f0707d28044faa7af58e1afe5854e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download is completed successfully\n",
            "Download is completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your google drive so files can be saved and retrieved permanently"
      ],
      "metadata": {
        "id": "6bEzpJfyl4g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awSYfXDbVlSd",
        "outputId": "0ea4ec8a-f24d-4bdf-daa0-d084afe3e563"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract and save audio from video stored in google drive"
      ],
      "metadata": {
        "id": "Qz6-Jry9mbm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import *\n",
        "\n",
        "# Path to the video file on Google Drive\n",
        "video_path = \"/content/function_calling.mp4\"\n",
        "\n",
        "# Load the video\n",
        "video = VideoFileClip(video_path)\n",
        "\n",
        "# Extract audio\n",
        "audio = video.audio\n",
        "\n",
        "# Path to save the output audio file\n",
        "audio_output_path = \"/content/function_calling.mp3\"\n",
        "\n",
        "# Save audio as MP3\n",
        "audio.write_audiofile(audio_output_path)\n",
        "\n",
        "# Close the clips\n",
        "video.close()\n",
        "audio.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU6W6M4tVxD1",
        "outputId": "63200589-571b-41f2-96a3-d50d2868d334"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/function_calling.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "whisper can only accept files of up to 25 mb, so a function is needed to check if the files need to be split up."
      ],
      "metadata": {
        "id": "my0WwKg2mua2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries and Define the check_size Function"
      ],
      "metadata": {
        "id": "7eBFBeuprIzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import openai\n",
        "\n",
        "# Function to check file size in MB\n",
        "def check_size(file_path):\n",
        "    return os.path.getsize(file_path) / (1024 * 1024)"
      ],
      "metadata": {
        "id": "NgwL_L7yrFOc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load and Optionally Compress Audio File"
      ],
      "metadata": {
        "id": "a80jTeTDrMjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize paths and list to store files for transcription\n",
        "audio_output_path = \"/content/function_calling.mp3\"\n",
        "compressed_audio_path = \"/content/compressed_audio.mp3\"\n",
        "files_to_transcribe = []\n",
        "\n",
        "# Check initial file size\n",
        "initial_size = check_size(audio_output_path)\n",
        "\n",
        "# Load original audio file\n",
        "audio = AudioSegment.from_file(audio_output_path)\n",
        "\n",
        "# Check if compression is needed\n",
        "if initial_size > 25:\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    audio.export(compressed_audio_path, format=\"mp3\", bitrate=\"48k\")\n",
        "    compressed_size = check_size(compressed_audio_path)\n",
        "else:\n",
        "    compressed_audio_path = audio_output_path\n",
        "    compressed_size = initial_size"
      ],
      "metadata": {
        "id": "JX0DPdJbrL1S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally Chunk the Audio File"
      ],
      "metadata": {
        "id": "T6L4zSQ3raSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if chunking is needed\n",
        "if compressed_size > 25:\n",
        "    num_chunks = int(compressed_size // 25) + 1\n",
        "    total_length = len(audio)\n",
        "    chunk_length = total_length // num_chunks\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_point = i * chunk_length\n",
        "        end_point = (i + 1) * chunk_length if i != num_chunks - 1 else total_length\n",
        "        chunk = audio[start_point:end_point]\n",
        "        chunk_path = f\"/content/chunk_{i+1}.mp3\"\n",
        "        chunk.export(chunk_path, format=\"mp3\")\n",
        "        files_to_transcribe.append(chunk_path)\n",
        "else:\n",
        "    files_to_transcribe.append(compressed_audio_path)"
      ],
      "metadata": {
        "id": "weVZyeu9rdcV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper Transcription"
      ],
      "metadata": {
        "id": "y5QsS3LLfEs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from ipywidgets import Password, Button, HBox, VBox\n",
        "from IPython.display import display\n",
        "\n",
        "# Create widgets\n",
        "api_key_input = Password(description='API Key:', placeholder='Enter your OpenAI API key')\n",
        "submit_button = Button(description='Submit')\n",
        "\n",
        "# Define function to execute on button press\n",
        "def store_api_key(b):\n",
        "    openai.api_key = api_key_input.value\n",
        "    print(\"API Key set!\")\n",
        "\n",
        "# Link button press to function\n",
        "submit_button.on_click(store_api_key)\n",
        "\n",
        "# Display widgets\n",
        "display(HBox([api_key_input, submit_button]))\n",
        "\n",
        "# Your existing code below\n",
        "# Initialize the transcripts dictionary\n",
        "transcripts = {}\n",
        "\n",
        "# Loop to transcribe files\n",
        "for idx, file_path in enumerate(files_to_transcribe):\n",
        "    with open(file_path, \"rb\") as audio_file:\n",
        "        transcripts[f\"transcript_{idx+1}\"] = openai.Audio.transcribe(\"whisper-1\", audio_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2c4140147f3f481084bd207ef5d9711b",
            "e7b997c88fa44252bdec96b20bac5d8b",
            "ad2d22217083437781b41a282e308636",
            "c0e1057d20994a57a4aa5bff5fe0a9fa",
            "0f0a83b152d4444f82efd70514a4c095",
            "24c50f08a2b547659b4d02d171cdc664",
            "30fb1c3bce00479a867997ded85b5fa0",
            "22a9dde7c8444b4ba80183ae072678cb"
          ]
        },
        "id": "fJ2UIvFAfK77",
        "outputId": "08b373e9-b4c1-4a6d-af91-a5c530324996"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Password(description='API Key:', placeholder='Enter your OpenAI API key'), Button(description='â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c4140147f3f481084bd207ef5d9711b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-35-774d05a274b2>\", line 27, in <cell line: 25>\n",
            "    transcripts[f\"transcript_{idx+1}\"] = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/audio.py\", line 72, in transcribe\n",
            "    response, _, api_key = requestor.request(\"post\", url, files=files, params=data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 289, in request\n",
            "    result = self.request_raw(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 606, in request_raw\n",
            "    result = _thread_context.session.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 790, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 461, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1274, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1130, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1624, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 871, in getmodule\n",
            "    if f == _filesbymodname.get(modname, None):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-774d05a274b2>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtranscripts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"transcript_{idx+1}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whisper-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/audio.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(cls, model, file, api_key, api_base, api_type, api_version, organization, deployment_id, **params)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transcriptions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         return util.convert_to_openai_object(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 289\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             result = _thread_context.session.request(\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcripts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKO8PaAktMbI",
        "outputId": "9de718d1-dd0e-43a5-c886-ebf0cfb49039"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'transcript_1': <OpenAIObject at 0x7d86724e0810> JSON: {\n",
            "  \"text\": \"Let's talk about GPT function calling. I think of GPT as this kind of brain that's floating out there in the cloud somewhere, and you can talk to it. You can talk to it through apps like ChatGPT, but you can also write code that talks to it through the API. So GPT functions were a recent addition, which are, well, they're super useful, but a little bit hard to wrap your head around sometimes. So I hope this video will make it crystal clear and you'll understand what it is and what you can use it for. So we're going to be doing some code examples. So here's our basic scaffolding, right? Some JavaScript, open AI key comes in there. We select a model there, and then we ask GPT this question and we log the answer. So let's try it. There we go. There's our generic, long, boring AI answer, but it's working. So the question is, what if we write something like this? Is it raining in Stockholm? So there we go. We get a long answer, basically boiling down to screw you. No, I will not do this. In fact, let's actually ask it to make short answers. There we go. That's a short answer. I don't know. So the reason for this is, of course, because of the sandbox, right? By default, GPT cannot access the internet or access your file system or access your email or anything really, which is probably for the better. Functions are really cool because they basically allow you to increase the capabilities of GPT, basically giving it a tool. So let's say I'm asking, is it raining in Stockholm? Then I can include, hey, by the way, there's a function called getWeather. And then GPT will say, oh, really? Well, I would like to use that function. And then I tell GPT, so the function returned this. And then GPT can give me the actual answer. So let's actually try this in code. Here we send the message, right? Is it raining in Stockholm? How do we tell GPT that there is a function available? Well, if we look at the documentation in the request body, you can include a list of functions that are available. And this is one of those things that, like in many cases, I would just tell GPT, write the code and it'd be done. But at the time of GPT's cutoff, it didn't have this information. This came later. So GPT doesn't actually know this stuff. You can give GPT this link and tell it to figure it out. But for the purpose of learning, I'll actually just look at the docs and show you where you can find it. And then we'll actually write the code. So we're going to add functions here. And that's where we define which functions we have. So what I'm putting here is a specification. I'm telling it about my function. And it uses the JSON schema reference. And I cobbled together a simple example of that by just looking around at the example code on the OpenAI documentation site and ended up with this. The function's name is weather. It gets the current weather for city. This part is important, by the way, because GPT uses this to determine when it should use the function or not. These are the properties it takes. And it takes one parameter called city, which is a string. Maybe this is redundant. I don't know. And here we say that city is, in fact, required. So this is a function definition, in a sense. And we're sending it to GPT and saying, this is actually available for you. Now let's see what happens if we call GPT. This is interesting, right? Look what happened. We got an answer, but no content in the answer. Instead, we get this thing, function call. It wants to call the function called weather, which we said exists. And it wants to send in the arguments, city, Stockholm. So what's happening now is exactly this right here. It's saying, hey, I want to call this function called getWeather. So let's update our code and actually do it. So it's important to note that GPT isn't actually executing that function. It is asking us to execute it and then send back the response. So let's actually create it. Fast forward a little bit. And I basically asked GPT to create a getWeather function for me. So openWeatherMap is a third-party API which you can use to check weather. And let's see how the output looks like. There we go. A pile of JSON describing the weather conditions in London. So back to the code. We ran this thing before. And we have GPT telling us, could you please make a function call to weather with these parameters? Okay. So here's some code for that. Response message, function call. We're basically parsing this. And we're noticing that, oh, it wants to call the weather function. And then we're parsing the arguments. And we're passing that on to our function. Let's see if it works. Yep. GPT asked me to call weather for city Stockholm. And indeed, here's the result. So we did that. We did this. We did that. And now we're about to do this. We're about to give the result back to GPT. So here, call GPT and give it the weather, right? The weather data. And that means we're going to do something like this again, right? We're going to call GPT. And to avoid duplicating this code, because we do want to keep the chat history, send that back to GPT, I'll just do a quick refactor and we'll put that into an array. So those are our messages. And we're going to just keep adding to this. This is our chat history, essentially, right? So we'll also add the response message to those messages. And then we can just kind of keep adding to this chat. So what we want to do now is we want to add more to it. I want to create an additional message. And what's actually going on here? Let's look at this, right? We sent a system message saying you give short answers. We sent a user message asking if it's raining in Stockholm. We got a response where it asked to call a function. And now we're sending a message with the role function. And this is a bit buried in the documentation. But if you look at the API docs, create chat completion, here on the messages, it says the role, one of system user assistant or function. That one, it's a little bit not obvious, but that's where you tell it the result of the function. So it's kind of like the function is speaking to GPT saying, hey, here's the result you're asking for. So we say this is the function that we called. And then we stringify the actual result. So this is the pile of JSON that the weather API spat out at us. But first, let's improve logging a little bit. There we go. And we run it. So first request there, as expected, got the response, call a function. And second request includes all the history plus the data. Great. So now let's actually send it to GPT. And let's log the result. All right. We have an answer. Nope. It is currently it is not currently raining in Stockholm. And of course, we could change the question, right? We can ask any question about weather now, basically. What's the wind in Paris? Why not? Boom. And it's going to check that. And it's blowing at a speed of 2.57 meters per second, etc. So we've kind of given GPT this tool. And GPT is choosing when it wants to use that tool. However, the way our code is written now, it's pretty limited. Because let's say the question is, is weather sunnier in Stockholm than in Paris? What's going to happen? Let's find out. As you can see, the response we got was not the final response. It was just another request to check weather, which kind of makes sense if you think about it. Because we gave it the tool, get weather, right? And we asked it about Stockholm and Paris. So first, it's like, I want to check the weather in Stockholm. And then we give it the answer. And then it's like, well, now I want to check the weather in Paris before I can give you an answer, which makes perfect sense. But our code will only handle one call to get weather. So let's actually fix that. We need to write some kind of loop that just keeps talking to GPT until it's kind of finished doing whatever it needs to do. And how do we know that? How do we know it's finished? Well, it turns out that there is deeply buried in the OpenAI documentation, this little thing here. Every response will include a finish reason. That's how we find out if GPT is done, and we kind of have an answer. So we need to change our code to make a loop. And this is a perfect situation where we could do it ourselves, but GPT will do it infinitely faster. Copy all the code, give it to GPT. Here you go. Here's a pile of code. And here's what I want to achieve. Use this code as a basis for making a function called callGPT. And then just keep going, keep calling the getWeather function whenever requested, and then return and log the final message when done. There we go. Let's see if it works. Replace all this. And let's just run it. All right, it worked. No, the weather in Stockholm is not sunnier than in Paris. And I won't dig through this in detail, but essentially it's doing what it's supposed to do. It's looping through this, talking to GPT, reacting to when GPT wants to call a getWeather function, and basically just running the loop that I talked about. And at the end, we get a result. So if we look at the code itself, I guess it's probably not rocket science. Let's have a quick look. The top is the same. We have our weather function. Here's our callGPT function, which is kind of the main function that we're calling from the bottom, right? CallGPT and ask about this thing. Let's just do this. It's easier to read. Is the weather sunny in Paris, in Stockholm than in Paris? And callGPT is just doing the loop. Send a message to GPT, get a response, check if that response is a request to call getWeather. If so, do so, and keep looping. And if at any point GPT says we're done, then we're done. We stop, right? So that's it. And we can, of course, change the question. Let's go, give me the weather in the three largest cities in Europe. And yeah, same thing. It does a loop and asks for getWeather as much as needed and then gives us a response. Now, this is pretty easy to extend if we want. So let's say we want to send a message to Now, this is pretty easy to extend if we want. So let's say we want to be able to save files as well. I'll be lazy and just ask GPT. So this is the kind of stuff where GPT does a really good job. The code is already there. I'm just doing more of the same kind of thing. I pasted in the updated code and let's have a quick look. There's old weather function spec and there's a new one. Save the file. So nothing complicated, really. You know, there's the file name and there's the and the helper function. And here we tell GPT that this is now available, right? We have the weather function spec and the save the file function spec. And then over down here, we also check, you know, are you trying to call save the file? If so, do it. So this should hopefully work. Let's test it. We can do, how about this? Let's do GPT four. And let's say make a file called song.txt, which contains a limerick about the current weather in Paris. So calling get weather. Calling save the file there. And then we're done. So there should be a file now. And there is. Song.txt. Okay. Not a great limerick. But you get the point. It is something like maybe a limerick and it is about Paris and it does include the weather. So I hope it's becoming a little more clear what functions are and how you use them. You might be thinking that, wait a sec, this is a bunch of code and there's some duplication in there and, you know, isn't there some more generic way of doing this? And of course there are. We could put all our functions in some kind of array. We could even use maybe reflection so that we don't have to manually write these specifications. There's all kinds of ways you can streamline this. But in this demo, I wanted to show what's actually going on beneath the hood. So that's why I'm kind of typing this out for you. But I want to show one more kind of use case for functions. So here we gave GPT functions and it used the functions to generate responses for us. But there is another simpler use case, which is easy to overlook. Let's say I want to know what is the main language spoken in the three largest cities in Europe. But I want the answer in a structured format because I want to put it inside a database or maybe send it off to some other API. Let's check what happens by default. So let's ask that specific question and there's the answer. So this format is arbitrary. GPT just picked something. We want a predictable format if we're going to be able to do something with this in terms of like putting in a database or something. So what can we do about that? One approach is to ask GPT please give it to me in a structured format. Typically, maybe I would write something like this. Okay, let's see if we can do this. We want a JSON format with the field city and language. All right, it looks like it created what I want. But just for in case, let's do, let's parse this as a JSON and see what it actually is, right? All right, was this what I expected? Not quite. I wasn't expecting this. Maybe I should provide a concrete example, right? That will probably work. But at the end of the day, this isn't really reliable. Even if you give a concrete example, you'll mostly get the right answer, but sometimes not. This is where function calling also comes into play because we can actually tell GPT that it has to use a function. We can say you have to use set languages in your response and then we can provide an exact structure for that. So let's try it. So this will actually be simpler code than before. There's no loop involved or anything. It's just one request and one response and then just save that response essentially. So I've prepared that code and this is what it looks like. We have a set languages function spec. The structure I expect to receive is an object that contains a field called cities, which is an array and each item in the array is a city name and a language spoken in that city. And those are both required. So again, there's tools to generate this pile of code. But this is what it looks like. I'm essentially defining a JSON schema here. And then in actual code, it's a lot simpler than before. The call GPT function. System message, you use a message just like before. What I added now is, well, there's this function, right? Set languages function spec. So I'm saying that here's a function that exists and then I'm going to saying that here's a function that exists. I'm saying you have to use it because that's what this means. And this is also one of those things that's kind of buried in the documentation. But here, if you want to force the model to call a specific function, you can do so by setting function call colon. This is a parameter that by default is set to auto, which means normally GPT will decide for itself if it should call a function or not. But in this case, the whole point of our request to GPT is that we wanted to call this function. So we'll force it. Function call name set languages. And like I said, this is not intuitive. So, you know, I hope this video will help you learn it a little faster than it took for me. But anyway, we check the response and we expect it to call this function. So if it doesn't, then that's an error, right? It really should. And then we just return the results. We parse that result and return it. So let's try. What is the main language spoken in the three largest cities in Europe? That's the request that I sent. And I don't need to specify what format I want because we've already said over here that we must call that function. So let's try. There we go. There's the result. Exactly as expected. I'm now in control of the format, which makes it a lot easier to use GPT as kind of an integration tool. Okay. I think that about sums it up. So what is GPT function calling? What is it useful for? I hope that has become more clear. And I gave you two use cases that have been very useful to me, at least. Augmenting GPT's capabilities and getting structured responses. There are probably tons more. So feel free to add your own suggestions in the comments. Okay. Thanks for listening. Bye.\"\n",
            "}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Combine Transcripts into a Master Transcript"
      ],
      "metadata": {
        "id": "iBdCKTjttBBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty string to store the master transcript\n",
        "master_transcript = \"\"\n",
        "\n",
        "# Loop through all keys in the 'transcripts' dictionary\n",
        "for idx, transcript_key in enumerate(transcripts.keys()):\n",
        "    # Extract the transcribed text stored under the key 'text_content'\n",
        "    transcript_text = transcripts[transcript_key]['text']\n",
        "\n",
        "    # Append the transcript_text to master_transcript\n",
        "    master_transcript += transcript_text + \" \"\n",
        "\n",
        "# Optionally, write the master transcript to a text file\n",
        "with open(\"/content/master_transcript.txt\", \"w\") as f:\n",
        "    f.write(master_transcript)"
      ],
      "metadata": {
        "id": "rUfoJxeAkzMV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(master_transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14VnzqFRuSt1",
        "outputId": "96cc2428-0bbb-4d71-a08a-a79c8ec7eb06"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's talk about GPT function calling. I think of GPT as this kind of brain that's floating out there in the cloud somewhere, and you can talk to it. You can talk to it through apps like ChatGPT, but you can also write code that talks to it through the API. So GPT functions were a recent addition, which are, well, they're super useful, but a little bit hard to wrap your head around sometimes. So I hope this video will make it crystal clear and you'll understand what it is and what you can use it for. So we're going to be doing some code examples. So here's our basic scaffolding, right? Some JavaScript, open AI key comes in there. We select a model there, and then we ask GPT this question and we log the answer. So let's try it. There we go. There's our generic, long, boring AI answer, but it's working. So the question is, what if we write something like this? Is it raining in Stockholm? So there we go. We get a long answer, basically boiling down to screw you. No, I will not do this. In fact, let's actually ask it to make short answers. There we go. That's a short answer. I don't know. So the reason for this is, of course, because of the sandbox, right? By default, GPT cannot access the internet or access your file system or access your email or anything really, which is probably for the better. Functions are really cool because they basically allow you to increase the capabilities of GPT, basically giving it a tool. So let's say I'm asking, is it raining in Stockholm? Then I can include, hey, by the way, there's a function called getWeather. And then GPT will say, oh, really? Well, I would like to use that function. And then I tell GPT, so the function returned this. And then GPT can give me the actual answer. So let's actually try this in code. Here we send the message, right? Is it raining in Stockholm? How do we tell GPT that there is a function available? Well, if we look at the documentation in the request body, you can include a list of functions that are available. And this is one of those things that, like in many cases, I would just tell GPT, write the code and it'd be done. But at the time of GPT's cutoff, it didn't have this information. This came later. So GPT doesn't actually know this stuff. You can give GPT this link and tell it to figure it out. But for the purpose of learning, I'll actually just look at the docs and show you where you can find it. And then we'll actually write the code. So we're going to add functions here. And that's where we define which functions we have. So what I'm putting here is a specification. I'm telling it about my function. And it uses the JSON schema reference. And I cobbled together a simple example of that by just looking around at the example code on the OpenAI documentation site and ended up with this. The function's name is weather. It gets the current weather for city. This part is important, by the way, because GPT uses this to determine when it should use the function or not. These are the properties it takes. And it takes one parameter called city, which is a string. Maybe this is redundant. I don't know. And here we say that city is, in fact, required. So this is a function definition, in a sense. And we're sending it to GPT and saying, this is actually available for you. Now let's see what happens if we call GPT. This is interesting, right? Look what happened. We got an answer, but no content in the answer. Instead, we get this thing, function call. It wants to call the function called weather, which we said exists. And it wants to send in the arguments, city, Stockholm. So what's happening now is exactly this right here. It's saying, hey, I want to call this function called getWeather. So let's update our code and actually do it. So it's important to note that GPT isn't actually executing that function. It is asking us to execute it and then send back the response. So let's actually create it. Fast forward a little bit. And I basically asked GPT to create a getWeather function for me. So openWeatherMap is a third-party API which you can use to check weather. And let's see how the output looks like. There we go. A pile of JSON describing the weather conditions in London. So back to the code. We ran this thing before. And we have GPT telling us, could you please make a function call to weather with these parameters? Okay. So here's some code for that. Response message, function call. We're basically parsing this. And we're noticing that, oh, it wants to call the weather function. And then we're parsing the arguments. And we're passing that on to our function. Let's see if it works. Yep. GPT asked me to call weather for city Stockholm. And indeed, here's the result. So we did that. We did this. We did that. And now we're about to do this. We're about to give the result back to GPT. So here, call GPT and give it the weather, right? The weather data. And that means we're going to do something like this again, right? We're going to call GPT. And to avoid duplicating this code, because we do want to keep the chat history, send that back to GPT, I'll just do a quick refactor and we'll put that into an array. So those are our messages. And we're going to just keep adding to this. This is our chat history, essentially, right? So we'll also add the response message to those messages. And then we can just kind of keep adding to this chat. So what we want to do now is we want to add more to it. I want to create an additional message. And what's actually going on here? Let's look at this, right? We sent a system message saying you give short answers. We sent a user message asking if it's raining in Stockholm. We got a response where it asked to call a function. And now we're sending a message with the role function. And this is a bit buried in the documentation. But if you look at the API docs, create chat completion, here on the messages, it says the role, one of system user assistant or function. That one, it's a little bit not obvious, but that's where you tell it the result of the function. So it's kind of like the function is speaking to GPT saying, hey, here's the result you're asking for. So we say this is the function that we called. And then we stringify the actual result. So this is the pile of JSON that the weather API spat out at us. But first, let's improve logging a little bit. There we go. And we run it. So first request there, as expected, got the response, call a function. And second request includes all the history plus the data. Great. So now let's actually send it to GPT. And let's log the result. All right. We have an answer. Nope. It is currently it is not currently raining in Stockholm. And of course, we could change the question, right? We can ask any question about weather now, basically. What's the wind in Paris? Why not? Boom. And it's going to check that. And it's blowing at a speed of 2.57 meters per second, etc. So we've kind of given GPT this tool. And GPT is choosing when it wants to use that tool. However, the way our code is written now, it's pretty limited. Because let's say the question is, is weather sunnier in Stockholm than in Paris? What's going to happen? Let's find out. As you can see, the response we got was not the final response. It was just another request to check weather, which kind of makes sense if you think about it. Because we gave it the tool, get weather, right? And we asked it about Stockholm and Paris. So first, it's like, I want to check the weather in Stockholm. And then we give it the answer. And then it's like, well, now I want to check the weather in Paris before I can give you an answer, which makes perfect sense. But our code will only handle one call to get weather. So let's actually fix that. We need to write some kind of loop that just keeps talking to GPT until it's kind of finished doing whatever it needs to do. And how do we know that? How do we know it's finished? Well, it turns out that there is deeply buried in the OpenAI documentation, this little thing here. Every response will include a finish reason. That's how we find out if GPT is done, and we kind of have an answer. So we need to change our code to make a loop. And this is a perfect situation where we could do it ourselves, but GPT will do it infinitely faster. Copy all the code, give it to GPT. Here you go. Here's a pile of code. And here's what I want to achieve. Use this code as a basis for making a function called callGPT. And then just keep going, keep calling the getWeather function whenever requested, and then return and log the final message when done. There we go. Let's see if it works. Replace all this. And let's just run it. All right, it worked. No, the weather in Stockholm is not sunnier than in Paris. And I won't dig through this in detail, but essentially it's doing what it's supposed to do. It's looping through this, talking to GPT, reacting to when GPT wants to call a getWeather function, and basically just running the loop that I talked about. And at the end, we get a result. So if we look at the code itself, I guess it's probably not rocket science. Let's have a quick look. The top is the same. We have our weather function. Here's our callGPT function, which is kind of the main function that we're calling from the bottom, right? CallGPT and ask about this thing. Let's just do this. It's easier to read. Is the weather sunny in Paris, in Stockholm than in Paris? And callGPT is just doing the loop. Send a message to GPT, get a response, check if that response is a request to call getWeather. If so, do so, and keep looping. And if at any point GPT says we're done, then we're done. We stop, right? So that's it. And we can, of course, change the question. Let's go, give me the weather in the three largest cities in Europe. And yeah, same thing. It does a loop and asks for getWeather as much as needed and then gives us a response. Now, this is pretty easy to extend if we want. So let's say we want to send a message to Now, this is pretty easy to extend if we want. So let's say we want to be able to save files as well. I'll be lazy and just ask GPT. So this is the kind of stuff where GPT does a really good job. The code is already there. I'm just doing more of the same kind of thing. I pasted in the updated code and let's have a quick look. There's old weather function spec and there's a new one. Save the file. So nothing complicated, really. You know, there's the file name and there's the and the helper function. And here we tell GPT that this is now available, right? We have the weather function spec and the save the file function spec. And then over down here, we also check, you know, are you trying to call save the file? If so, do it. So this should hopefully work. Let's test it. We can do, how about this? Let's do GPT four. And let's say make a file called song.txt, which contains a limerick about the current weather in Paris. So calling get weather. Calling save the file there. And then we're done. So there should be a file now. And there is. Song.txt. Okay. Not a great limerick. But you get the point. It is something like maybe a limerick and it is about Paris and it does include the weather. So I hope it's becoming a little more clear what functions are and how you use them. You might be thinking that, wait a sec, this is a bunch of code and there's some duplication in there and, you know, isn't there some more generic way of doing this? And of course there are. We could put all our functions in some kind of array. We could even use maybe reflection so that we don't have to manually write these specifications. There's all kinds of ways you can streamline this. But in this demo, I wanted to show what's actually going on beneath the hood. So that's why I'm kind of typing this out for you. But I want to show one more kind of use case for functions. So here we gave GPT functions and it used the functions to generate responses for us. But there is another simpler use case, which is easy to overlook. Let's say I want to know what is the main language spoken in the three largest cities in Europe. But I want the answer in a structured format because I want to put it inside a database or maybe send it off to some other API. Let's check what happens by default. So let's ask that specific question and there's the answer. So this format is arbitrary. GPT just picked something. We want a predictable format if we're going to be able to do something with this in terms of like putting in a database or something. So what can we do about that? One approach is to ask GPT please give it to me in a structured format. Typically, maybe I would write something like this. Okay, let's see if we can do this. We want a JSON format with the field city and language. All right, it looks like it created what I want. But just for in case, let's do, let's parse this as a JSON and see what it actually is, right? All right, was this what I expected? Not quite. I wasn't expecting this. Maybe I should provide a concrete example, right? That will probably work. But at the end of the day, this isn't really reliable. Even if you give a concrete example, you'll mostly get the right answer, but sometimes not. This is where function calling also comes into play because we can actually tell GPT that it has to use a function. We can say you have to use set languages in your response and then we can provide an exact structure for that. So let's try it. So this will actually be simpler code than before. There's no loop involved or anything. It's just one request and one response and then just save that response essentially. So I've prepared that code and this is what it looks like. We have a set languages function spec. The structure I expect to receive is an object that contains a field called cities, which is an array and each item in the array is a city name and a language spoken in that city. And those are both required. So again, there's tools to generate this pile of code. But this is what it looks like. I'm essentially defining a JSON schema here. And then in actual code, it's a lot simpler than before. The call GPT function. System message, you use a message just like before. What I added now is, well, there's this function, right? Set languages function spec. So I'm saying that here's a function that exists and then I'm going to saying that here's a function that exists. I'm saying you have to use it because that's what this means. And this is also one of those things that's kind of buried in the documentation. But here, if you want to force the model to call a specific function, you can do so by setting function call colon. This is a parameter that by default is set to auto, which means normally GPT will decide for itself if it should call a function or not. But in this case, the whole point of our request to GPT is that we wanted to call this function. So we'll force it. Function call name set languages. And like I said, this is not intuitive. So, you know, I hope this video will help you learn it a little faster than it took for me. But anyway, we check the response and we expect it to call this function. So if it doesn't, then that's an error, right? It really should. And then we just return the results. We parse that result and return it. So let's try. What is the main language spoken in the three largest cities in Europe? That's the request that I sent. And I don't need to specify what format I want because we've already said over here that we must call that function. So let's try. There we go. There's the result. Exactly as expected. I'm now in control of the format, which makes it a lot easier to use GPT as kind of an integration tool. Okay. I think that about sums it up. So what is GPT function calling? What is it useful for? I hope that has become more clear. And I gave you two use cases that have been very useful to me, at least. Augmenting GPT's capabilities and getting structured responses. There are probably tons more. So feel free to add your own suggestions in the comments. Okay. Thanks for listening. Bye. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain of density"
      ],
      "metadata": {
        "id": "673QPKpmlCDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement chain of density prompting\n",
        "from typing import List\n",
        "from dataclasses import dataclass\n",
        "from llm_core.assistants import OpenAIAssistant\n",
        "\n",
        "@dataclass\n",
        "class DenseSummary:\n",
        "    denser_summary: str\n",
        "    missing_entities: List[str]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DenserSummaryCollection:\n",
        "  system_prompt = \"\"\"\n",
        "  You are an expert in writing rich and dense summaries in broad domains.\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = \"\"\"\n",
        "\n",
        "  Context:\n",
        "\n",
        "  The following speech is a youtube lecture on Open AI function calling.\n",
        "\n",
        "\n",
        "  ----\n",
        "  Speech:\n",
        "\n",
        "  {article}\n",
        "\n",
        "  ----\n",
        "\n",
        "  You will generate increasingly concise, entity-dense summaries of the speech snippet. You will use any context provided to make sense of and orient the\n",
        "  summary of the speech with what has come before.\n",
        "\n",
        "  Repeat the following 2 steps 5 times.\n",
        "\n",
        "  - Step 1: Identify 1-3 informative Entities from the Article\n",
        "  which are missing from the previously generated summary and are the most\n",
        "  relevant.\n",
        "\n",
        "  - Step 2: Write a new, denser summary of identical length which covers\n",
        "  every entity and detail from the previous summary plus the missing entities\n",
        "\n",
        "  A Missing Entity is:\n",
        "\n",
        "  - Relevant: to the main story\n",
        "  - Specific: descriptive yet concise (5 words or fewer)\n",
        "  - Novel: not in the previous summary\n",
        "  - Faithful: present in the Article\n",
        "  - Anywhere: located anywhere in the Article\n",
        "\n",
        "  Guidelines:\n",
        "  - Use the context provided to make sense of the article and ensure that there\n",
        "  is consistency between the information provided in the context and the\n",
        "  information provided in the article.\n",
        "\n",
        "  - Do not repeat points made in the speech if they are already provided in the context.\n",
        "\n",
        "  - Only add new pieces of information, make sure nothing mentioned in the context is repeated.\n",
        "\n",
        "  - The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
        "  highly non-specific, containing little information beyond the entities\n",
        "  marked as missing.\n",
        "\n",
        "  - Use overly verbose language and fillers (e.g. \"this article discusses\") to\n",
        "  reach approx. 80 words.\n",
        "\n",
        "  - Make every word count: re-write the previous summary to improve flow and\n",
        "  make space for additional entities.\n",
        "\n",
        "  - Make space with fusion, compression, and removal of uninformative phrases\n",
        "  like \"the article discusses\"\n",
        "\n",
        "  - The summaries should become highly dense and concise yet self-contained,\n",
        "  e.g., easily understood without the Article.\n",
        "\n",
        "  - Missing entities can appear anywhere in the new summary.\n",
        "\n",
        "  - Never drop entities from the previous summary. If space cannot be made,\n",
        "  add fewer new entities.\n",
        "\n",
        "  > Remember to use the exact same number of words for each summary.\n",
        "  Answer in JSON.\n",
        "\n",
        "  > The JSON in `summaries_per_step` should be a list (length 5) of\n",
        "  dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  summaries: List[DenseSummary]\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def summarize(cls, article):\n",
        "      with OpenAIAssistant(cls, model='gpt-4') as assistant:\n",
        "          return assistant.process(article=article)"
      ],
      "metadata": {
        "id": "eirZSRg8mrOo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = DenserSummaryCollection.summarize(master_transcript)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8t0v7cDwJTb",
        "outputId": "f8f6a526-ec1b-4960-bedc-3bc24a733273"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenserSummaryCollection(summaries=[DenseSummary(denser_summary=\"The speaker discusses the concept of GPT function calling in a YouTube lecture. He explains how GPT functions can be used to enhance the capabilities of GPT, such as asking about the weather in a specific city. The speaker also demonstrates how to write code that interacts with GPT through the API. He further illustrates how to define functions and how GPT can use these functions to generate responses. The speaker also highlights the importance of the sandbox in GPT, which prevents it from accessing the internet or the user's file system.\", missing_entities=['YouTube lecture', 'GPT function calling', 'API']), DenseSummary(denser_summary=\"In a YouTube lecture, the speaker elucidates GPT function calling, demonstrating how to use the API to enhance GPT's capabilities. He shows how to define functions like 'getWeather', which GPT can utilize to generate responses. He emphasizes the sandbox's role in GPT, preventing it from accessing the internet or user's file system. He also explains how to instruct GPT to provide short answers and how to handle multiple function calls.\", missing_entities=['getWeather function', 'short answers', 'multiple function calls']), DenseSummary(denser_summary=\"The speaker, in a YouTube lecture, elucidates GPT function calling, demonstrating how to use the API to enhance GPT's capabilities. He shows how to define functions like 'getWeather', which GPT can utilize to generate responses. He emphasizes the sandbox's role in GPT, preventing it from accessing the internet or user's file system. He also explains how to instruct GPT to provide short answers and how to handle multiple function calls. He further demonstrates how to use a loop to keep calling the 'getWeather' function until GPT is done.\", missing_entities=['use of loop', 'getWeather function', 'GPT done']), DenseSummary(denser_summary=\"In a YouTube lecture, the speaker elucidates GPT function calling, demonstrating how to use the API to enhance GPT's capabilities. He shows how to define functions like 'getWeather', which GPT can utilize to generate responses. He emphasizes the sandbox's role in GPT, preventing it from accessing the internet or user's file system. He also explains how to instruct GPT to provide short answers and how to handle multiple function calls. He further demonstrates how to use a loop to keep calling the 'getWeather' function until GPT is done and how to get structured responses from GPT.\", missing_entities=['structured responses', 'getWeather function', 'GPT done']), DenseSummary(denser_summary=\"In a YouTube lecture, the speaker elucidates GPT function calling, demonstrating how to use the API to enhance GPT's capabilities. He shows how to define functions like 'getWeather', which GPT can utilize to generate responses. He emphasizes the sandbox's role in GPT, preventing it from accessing the internet or user's file system. He also explains how to instruct GPT to provide short answers and how to handle multiple function calls. He further demonstrates how to use a loop to keep calling the 'getWeather' function until GPT is done, how to get structured responses from GPT, and how to force GPT to call a specific function.\", missing_entities=['force GPT to call a specific function', 'structured responses', 'getWeather function'])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary.summaries[4].denser_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "79Ve9A0EwkVK",
        "outputId": "facc2882-272f-4697-80c4-b7404289de92"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre>In a YouTube lecture, the speaker elucidates GPT function calling, demonstrating how to use the API to enhance GPT's capabilities. He shows how to define functions like 'getWeather', which GPT can utilize to generate responses. He emphasizes the sandbox's role in GPT, preventing it from accessing the internet or user's file system. He also explains how to instruct GPT to provide short answers and how to handle multiple function calls. He further demonstrates how to use a loop to keep calling the 'getWeather' function until GPT is done, how to get structured responses from GPT, and how to force GPT to call a specific function.</pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "juaVTfRtrHgB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}